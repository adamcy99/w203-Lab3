---
title: "W203 Lab 3"
author: "Armand Kok, Adam Yang, James De La Torre"
output:
  pdf_document:
    fig_height: 4
    fig_width: 6
  html_document:
    df_print: paged
  'pdf_document:': default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
#opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```



# Introduction

**Is the introduction clear? Is the research question specific and well defined? Could the research question
lead to an actionable policy reccomendation? Does it motivate the analysis? Note that we’re not necessarily
expecting a long introduction. Even a single paragraph is probably enough for most reports.**


Our team has been hired by a local political campaign to provide research on North Carolina crime statistics and to generate policy suggestions for reducing crime. Our candidate seeks to portray herself as being “pro-cop” and “tough on crime”, and she espouses strong policing and enforcement.  She also has a strong desire to understand the situations faced by the minority population within the state, and she has expressed a keen interest in understanding how minority communities are impacted by crime.

The crime statistics dataset provided for analysis is a subset of the data used by Cornwell and W. Trumball in their 1994 study. The dependent variable, of our study is the crimes commited per capita, given as `crmrate`, while there are 24 other variables in the dataset, each of which can be potential modulators of the crime rate. We aim to build a linear model that regresses `crmrate` on the key variables in the dataset. In particular, we are interested in examining the potential of the following policies in reducing crime rate:
  * Policy to increase the police per capita of a county
  * Policy to implement a more stringent arrest protocol
  * Policy to enhance community outreach in high density and minority communities

In addition, we aim to identify other factors that may reduce crime and attempt to fully explore other possible political strategies. Not all correlating variables will have an actionable solution, though their inclusion in the regression model will contribute to its accuracy.


# 2.0 Data Loading and Cleaning

TO DO: Look for any top-coding or bottom coding.
TO DO: remove this instructiuon line
**Did the team notice any anomalous values? Is there a sufficient justification for any data points that are
removed? Did the report note any coding features that affect the meaning of variables (e.g. top-coding or
bottom-coding)? Overall, does the report demonstrate a thorough understanding of the data?**


The data provided is a sample from 91 counties in North Carolina, containing information from 1987.  The variables in the dataset and their meanings are shown below:

|**Variable**|**Label**|**Variable**|**Label**|
|-------|----|------|--------|
|**county**|county identifier   |**urban**|=1 if in SMSA|
|**year**|1987   |**pctmin80**|perc. minority, 1980|
|**crmrte**|crimes committed per person   |**wcon**|weekly wage, construction|
|**prbarr**|'probability' of arrest **\***   |**wtuc**|wkly wge, trns, util, commun|
|**prbconv**|'probability' of conviction **\***   |**wtrd**|wkly wge, whlesle, retail trade|
|**prbpris**|'probability' of prison sentence **\***   |**wfir**|wkly wge, fin, ins, real est|
|**avgsen**|avg. sentence, days   |**wser**|wkly wge, service industry|
|**polpc**|police per capita   |**wmfg**|wkly wge, manufacturing|
|**density**|people per sq. mile   |**wfed**|wkly wge, fed employees|
|**taxpc**|tax revenue per capita   |**wsta**|wkly wge, state employees|
|**west**|=1 if in western N.C.   |**wloc**|wkly wge, local gov emps|
|**central**|=1 if in central N.C.   |**mix**|offense mix: face-to-face/other|
|**pctymle**|percent young male|

**\*** *These are not true probabilities that are limited between 0 and 1, but are rations instead. For example, `probconv` is the ratio of the number of convictions to the number of arrests, which can be larger than 1.*


## 2.1 Loading the Data


The data file, `crime_v2.csv` was opened and found to contain 97 rows.

```{R}
# Import all libraries that will be used in the lab
library(car)
library(reshape2)
library(ggplot2)
library(stargazer)
library(sandwich)
library(lmtest)

# Adam's dir
#mydir <- "/Users/adamyang/Desktop/w203/Lab3/w203-Lab3/"

# Armand's dir
mydir<-"C:/Users/ak021523/Documents/GitHub/mids-repos/W203/Homework/w203-Lab3/"

#jim's directory
#mydir<- "F:/users/jddel/Documents/DATA_SCIENCE_DEGREE_LAPTOP/W203_Stats/Lab_03/"

# read df
crime_df = read.csv(paste0(mydir,'crime_v2.csv'))
```

## 2.2 Data Cleanup

Immediate inspection of the data revealed a few data cleanup steps were required.

- The last 6 rows of the data set were blanks.  These empty records were deleted.
- One row had values of 1 for both `west` and `central`, placing that county in two regions simultaneously.  It is unknown whether this is possible, but currently there has been no reason to delete this particular row so the data will be kept for now, as evaluation of variable importance is still ongoing.
- The `prbconv` variable, representing the "probability of conviction" was read in as a factor (a cateogorical variable) instead of a numeric variable.  This variable was converted to numeric.

```{R}

# summarize all vars
summary(crime_df)

str(crime_df)


# get rid of rows with missing values (this only kills the 6 blank rows)
crime_df<-crime_df[complete.cases(crime_df),]

# convert prob of conviction to numeric
crime_df$prbconv <- as.numeric(as.character(crime_df$prbconv))
```


## 2.3 Outlier Identification

TO DO: Write function that computes outliers by column

After reviewing the distributions of the different variables, there were 4 variables had outliers, which is defined by anything that is more than Q3 + 1.5 IQR or Q1 - 1.5 IQR: 
- polpc - row 51
- prbarr - row 51
- wser - row 84
- taxpc row 25
After reviewing further, there was no reason for the extreme outliers to be removed from the data set. boxplots of the variables above are shown below.

```{R}

boxplot(crime_df$polpc, main ='polpc')
boxplot(crime_df$prbarr, main ='prbarr')
boxplot(crime_df$wser, main = 'wser')
boxplot(crime_df$taxpc, main = 'taxpc')

# 1.5 IQR from the Q3 = outlier but we can decide which to eliminate
```



# 3.0 Model Building Process

TO DO: remove this instruction text


*Overall, is each step in the model building process supported by EDA? Is the outcome variable (or variables) appropriate? Is there a thorough univariate analysis of the outcome variable. Did the team identify at least two key explanatory variables and perform a thorough univariate analysis of each? Did the team clearly state why they chose these explanatory variables, does this explanation make sense in term of their research question? Did the team consider available variable transformations and select them with an eye towards model plausibility and interperability? Are transformations used to expose linear relationships in scatterplots? Is there enough explanation in the text to understand the meaning of each visualization?*


TO DO: Mention we looked at histograms of each variable
TO DO: Figure out where we want to mention statistical significance of calculated coefficients.  At end of each model section? after producing all models?



## 3.1 Check for multicolinearity

TO DO: clean up this sentence

To understand the correlation of each variable in the dataset to crime rate and to detect any collinear relationships between explanatory variables, a correlation matrix was constructed as shown below.  This will be useful information as additional variables are added to intial models.

Build a correlation matrix.
Identify input variables that correlate with one another. 
Choose only one variable from each correlated pair to include in model-building.

```{R}
#TODO - fix matrix sizing

#correlation matrix for top 4 correlation and bottom 4 correlation
cor_dr = cor(crime_df[c('prbarr',
                        'prbpris',
                        'prbconv',
                        'avgsen',
                        'polpc',
                        'density',
                        'taxpc',
                        'west',
                        'central',
                        'urban',
                        'pctmin80',
                        'wcon',
                        'wtuc',
                        'wtrd',
                        'wfir',
                        'wser',
                        'wmfg',
                        'wfed',
                        'wsta',
                        'wloc',
                        'mix',
                        'pctymle')],
              use = "complete.obs")

# Heatmap
ggplot(data = melt(cor_dr, na.rm = TRUE),
       aes(Var2, Var1, fill = value))+
theme_minimal()+ 
geom_tile(color = 'white')+
scale_fill_gradient2(low = 'blue',
                     high = "orange",
                     mid = 'white', 
                     midpoint = 0,
                     limit = c(-1,1),
                     name= 'Correlation')+
theme(axis.text.x = element_text(face='bold',
                                 angle=90,
                                 vjust = 1,
                                 size =8,
                                 hjust = 1),
     axis.text.y = element_text(face='bold',
                                size=8),
     axis.title.x=element_blank(),
     axis.title.y=element_blank())

```
One of the assumptions for multiple OLS regression is to avoid perfect multicollinearity between independent variables. This, however, is not common in practical cases. Less than perfect multicollinearity is a more common problem that will not cause bias in the OLS, but would introduce large variances and covariances. As a result, precise estimation would become difficult so it can be beneficial to remove certain imperfect multicollinearity variables.

After reviewing the correlation matrix in detail, there were 5 pairs of variables that have a somewhat strong correlation to each other (i.e. has correlation > 0.6), which are plotted below. Based on the plots, then the following variables were removed from the final model:
- urban - this is somewhat redundant with density.
- west - west was removed because it is a dummy variable, and pctmin80 is a continuous one which may contain more information for the regression model.
- wtrd, wfed, wfir - wages tend to be higher with density, so density was kept as it can succinctly represent the same information. Below are the scatterplots of the different correlated variables

```{R}
plot(crime_df$urban, crime_df$density)
plot(crime_df$west, crime_df$pctmin80)
plot(crime_df$wtrd, crime_df$wfir)
plot(crime_df$wtrd, crime_df$wfed)
plot(crime_df$wfed, crime_df$wfir)
plot(crime_df$wfed+crime_df$wtrd+crime_df$wfir, crime_df$density)
```


## 3.2 Scatterplot Matrix

To visualize the relationship between crime rate and our explanatory variables of interest, a scatterplot matrix was generated.


```{R}

spm(~crmrte + polpc + prbarr + density + pctmin80, data=crime_df)
``` 


The plots reveal that each of the selected explanatory variables shows a relationship with crime rate.  There is some degree of nonlinear relationship between `polpc` and `crmrte` and between `pctmin80` and `crmrte`.  However, transforming these variables would distort the practical interpretability of any model slope coefficients.  Therefore, the variables will not be transformed.









# 4.0 Regression Models: Base Model

The initial model created contains only those variables directly related to the candidate's positions on being pro-police, for strict enforcement, and concern with inner city and minority communities. Therefore, the variables we have chosen to represent these positions are: probability of arrest (prbarr), density, police per capita (polpc), and the percentage of minorities (pctmin80). 

```{R}
# Creating initial model
model1 <- lm(crmrte ~ prbarr + density + polpc + pctmin80 , data = crime_df)
```

After creating the model, we will start by evaluating it against the six Classical Linear Model assumptions.

**CLM 1. Linear population model:** We do not have to worry about this assumption at the moment because we haven’t constrained the error term. 

**CLM 2. Random Sampling:** To check random sampling, we need domain knowledge and an understanding of how the data were collected.  There are 100 counties in North Carolina, and there are data for 91 of them.  Without knowledge of the 9 excluded counties, no statement regarding the validity of random sampling can be made.

**CLM 3. No perfect multicollinearity:** There is no need to explicitly check for perfect collinearity, because R would've reported a warning if this occurred.  Furthermore, the correlation matrix shown in section "__TO DO____" also shows that there is no perfect collinearity.

**CLM 4. Zero Conditional Mean:**  $E(u|x)=0$. For this model, the residuals vs. fitted values plot shown below reveals a relatively flat spline centered around zero. Therefore, there does not seem to be a clear deviation from the zero conditional mean and the assumption holds.

```{R}
# Residuals vs. Fitted Plot
plot(model1, which=1)
```

**CLM 5. Homoscedasticity:** In the residuals vs. fitted values plot shown in **CLM 4**, the data points seem to form a cone shape which suggests some heteroscedasticity. In the scale-location plot below, there seems to be a slight positive slope across the range of fitted values between 0.02 and 0.04. Furthermore, the Breusch-Pagan test shown below has a p-value of 6.278e-05 which indicates that the null hypothesis of homoscedasticity can be rejected.  When evaluating the statistical significance of calculated model coefficients, heteroscedastic-robust standard errors will be used.

```{R}
# Scale-Location Plot
plot(model1, which=3)

# Breusch-Pagan
bptest(model1)
```

**CLM 6. Normality of errors:** In the Q-Q plot shown below, the bulk of the error terms seem to follow the straight line which suggests a fairly normal distribution. However, the standardized residuals show some deviation from the straight line at the extreme ends of the distribution. This suggests some skew at the extreme ends of our residuals. Furthermore, the Shapiro test shown below has a p value of 0.0002 which means we can reject the null hypothesis of the residuals having a normal distribution.

```{R}
# Q-Q plot of Standardized Residuals
plot(model1, which=2)

shapiro.test(model1$residuals)
```

To further verify this observation, a histogram of this model's residuals is shown below. The histogram shows approximate normality near the center of the distribution, but also some evidence of skewness; especially on the positive end. However, the Central Limit Theorem (CLT) claims that if the sample size is large enough we can assume that the residuals have a normal sampling distribution. For distributions with a very strong skew, a much larger sample size may be required, but for minor skews as in this case, the rule of thumb is that the CLT can be applied when the sample size is greater than 30. The sample size used for this model is 91 which should be enough for the CLT to hold.

```{R}
hist(model1$residuals, breaks=20)
```

Based on our review of the six CLM assumptions, this is a valid linear model. We replaced the regular standard errors with the heteroskedasticity-robust standard errors. The resulting coefficients and parameters of the model are shown below:

```{R}

# TO DO: Use this at the end (section 4.3)
# TO DO: add F-statistics and comment on it, code below
#linearHypothesis(model1, c("prbarr = 0", " density = 0", "polpc = 0", "pctmin80 = 0"), vcov = vcovHC)

# Replace regular Standard Errors with the heteroskedasticity-robust Standard Errors
#se.model1 <- sqrt(diag(vcovHC(model1)))

#stargazer(model1, title = "Base Model",type = "text", report = "vcstp", omit.stat = "f", se = list(se.model1, NULL), star.cutoffs = c(0.05,0.01,0.001))

paste("adj.r.square:", summary(model1)$adj.r.squared)
coeftest(model1, vcovHC)

```

The adjusted r-squared of the model is relatively high at 0.66. This means that 66% of the variation in crime rate is explained by our input variables. Furthermore, the results of our initial model shows that the probability of arrest is statistically significant as a modulator of crime, while the density and minority percentage of each county are strongly statistically significant. The police per capita, on the other hand, is not. The slope coefficients tell us that for every 1 unit increase in `prbarr`, there is a corresponding 0.046 decrease in the crime rate. The model also suggests that by increasing the density of a county by 1 person per square mile, crime commited per person may rise by 0.008. Finally, for every percentage point increase of minorities in a county, crime commited per person may rise by 0.0003. The model also suggests that by increasing the police per capita by 1 will result in 5 additional crimes commited per person. However, this slope coefficeint is shown to be statistically insignificant.

To further assess the strength of our model, we can take a look at the residuals vs. leverage plot shown below. Here we can see that data point 51, has a Cook's distance greater than 1, meaning it has high influence over the model. As shown in section **2.3 TO DO** this data point has `polpc` and `prbarr` values multiple times higher than the next highest values for these variables. If this data point is not representative of the general population in North Carolina, then it may hurt the accuracy of our model. However, we investigated the other values of this county and could not justify removing this data point without further information.

Furthermore, a general rule is that if 1 % (or more) data points have standardized residuals > 2.5, the model contains too much error. If 5% (or more) of data points have residuals > 2, the model has too much error and represents our data poorly. In the residual vs. leverage plot below, we see that 7.7% of our data points have standardized residuals over 2. Therefore, our model has too much error and may represent our data poorly.

Because of this, we will now incorporate a few covariates that might increase the accuracy of our results. 

```{R}
plot(model1, which=5)
```

## 4.1 Regression Model: Second Model

*Does this model include covariates meant to increase the accuracy of the regression? Has the team justified inclusion of each of these additional variables? Does the team identify what they want to measure with each coefficient? Does the team interpret the result of the regression in a thorough and convincing manner. Does the team evaluate all 6 CLM assumptions? Are the conclusions they draw based on this evaluation appropriate? Did the team interpret the results in terms of their research question?*

*One model that includes key explanatory variables and only covariates that you believe increase the accuracy of your results without introducing substantial bias (for example, you should not include outcome variables that will absorb some of the causal effect you are interested in). This model should strike a balance between accuracy and parsimony and reflect your best understanding of the determinants of crime.*



```{R}

# new: prbconv pctymle
model2 <- lm(crmrte ~ prbarr + density + polpc + pctmin80 + prbconv + pctymle, data = crime_df)
plot(model2)

# Breusch-Pagan
# 0.0004972

```

TO DO: Fill in discussion of CLM assumptions



# 4.2 Regression Third model

TO DO: add a lot of variables.  Consider colinearity that we discovered in correlation matrix in modeling section.
is the increase signficant from model1 -> model2 -> model3

```{R}
#TO DO: Joint significance testing
#joint significance test1
#rest_model = model1, unrstr_model = model2

#joint significance test2
#rest_model = model2, unrstr_model = model3


#model 3
model3 <- lm(crmrte ~ prbarr + density + polpc + pctmin80 + log(wcon) + log(wtuc) + log(wtrd) + log(wfir) + log(wser) + log(wmfg) + log (wsta) + log(wloc), data = crime_df)

#adjusted r-squared
summary(model3)$adj.r.squared

plot(model3)

#waldtest since we have heteroskadsticity
waldtest(model1, model3, vcov = vcovHC)

#test coefficient significance
coeftest(model3, vcov=vcovHC)

```




# 4.3 Regression Table

TO DO: Be sure to convert SE's to robust before displaying.































The following is the model that contains almost all available variables as explanatory variables with the exception of variables we excluded due to potential multi-collinearity.

```{R}
crime_df2 <- crime_df[-c(84,25),]

model1 <- lm(crmrte ~ . - county-year-crmrte-urban-west-wtrd-wfed-wfir, data = crime_df2)

summary(model1)$r.squared
summary(model1)$coefficients

```

The following is the model that contains a transformed explanatory variable.

```{R}
model_transform <- lm(crmrte ~ prbarr + log(prbconv) + density, data = crime_df2)

summary(model_transform)$r.squared
summary(model_transform)$coefficients
```

The following is the model that contains only variables that were identified to be most relevant to crmrte based on their marginal R-squared and standardized slope coefficient values.

```{R}
model_key <- lm(crmrte ~ prbarr + prbconv + polpc + density + pctmin80, data = crime_df2)

summary(model_key)$r.squared
summary(model_key)$coefficients
```

### Stargazer Regression Table for Model Specifications

```{R, results='asis'}
library(stargazer)
stargazer(model_transform, model_key, model1,
          title = "Linear Models Parameters Predicting Crime Rate",
          type = 'text', report = 'vc',
          keep.stat = c('rsq', 'n'),
          omit.table.layout = 'n')
```


### Recommendation

For interpretability purposes, the model was re-done using non-standardized variables:
-prbarr
-prbconv
-polpc
-density
-pctmin80

Recommendation for political campaign:
- police per capita has a positive slope coefficient with crmrte, and this may be due to more police are present in areas with high crmrte. This suggests that purely hiring more police officers may not be an impactful solution.
- However probability of arrest and conviction both have a negative slope coefficients. The model suggests that perhaps a zero tolerance policy towards crime is needed to increase arrests and convictions and thus deter crimes from happening.
- In terms areas with large minority population and high density, since these variable cannot be changed that much, perhaps a community outreach (e.g. job training program, afterschool programs, tutor/mentor program) to educate areas with a lot of minority can be done, so that crimes can be reduced in those areas.


### Omitted Variables

Potential Omitted Variable #1: poverty_rate
$$ crmrte = \beta_0 + \beta_1*density + \beta_2*poverty\_rate + u$$
$$ poverty\_rate = \alpha_0 + \alpha_1*density +u $$

- One thing that was noticeable in  the data is that crmrate was higher in dense areas and large minority population, however this may be due to an omitted variable that is not available in the data set. 
- For example: in dense areas the cost of living may be much higher, which can explain why higher wages are correlated with dense areas, but because of the higher cost of living. Because of this, there may be a lot more people living under the poverty line, which would encourage them to commit crimes and hence why dense areas have higher crmrte.
- so the density slope coefficient in this instance is probably higher than it should be $\beta_2$ and $\alpha_1$ would be positive.
- Maybe tax revenue or wages can help proxy this omitted variable.

Potential Omitted Variable #2: discrimination
$$ crmrte = \beta_0 + \beta_1*pctmin80 + \beta_2*discrimination $$
$$ discrimination = \alpha_0 + \alpha_1*pctmin80 $$
- Similarly minorities may be arrested for crimes more often than necessary due to discrimination. 
- in this scenario $\beta_2$ and $alpha_1$ would be a positive value.

Potential Omitted Variable #3: raised_in_oneparent_hh
$$ crmrte = \beta_0 + \beta_1*pctmin80 + \beta_2*raised\_in\_2parents\_hh $$
$$ raised\_in\_2parents\_hh = \alpha_0 + \alpha_1*pctmin80 $$
- In this scenario, minorities may be more likely to be raised in a single parent house hold. Thus making them more likely to commit crimes. 
- $\beta_2$ would be positive and $\alpha_1$ would be negative.

Potential Omitted Variable #4: unemployment

$$ crmrte = \beta_0 + \beta_1*density + \beta_2*unemployment $$

$$ unemployment = \alpha_0 + \alpha_1*density $$

- Higher umployment = higher crime rate (beta2 > 0)
- Higher density = higher unemployment (alpha1 > 0)
- beta1 was positive, therefore, it might be higher than it should've been.

Potential Omitted Variable #5: years_of_education

$$ crmrte = \beta_0 + \beta_1*pctmin80 + \beta_2*years\_of\_education $$
$$ years\_of\_education = \alpha_0 + \alpha_1*pctmin80 $$
- Higher avg years of education for a county would result in lower crime rate, beta2 < 0
- Higher percentage of minorities = lower average years of education for a county, alpha1 <0
- $\beta_2*\alpha_1 > 0$, beta1 >0, therefore, it might be higher than it should've been.



# TO BE SORTED LATER
# TO BE SORTED LATER
# TO BE SORTED LATER
# TO BE SORTED LATER


### Standardized Regression Model

TO DO: Eliminate. Save the comments on the diagnostic plots for use in the non-standardized model analysis.

A multi variable regression model was created using the data set that has been standardized above. 

Then the model was evaluated for potential high leverage/influence data points as well as potential biases.

In review the following findings were noted:
- row 84 and 25 have a high Cook's distance and high standardized residuals, which means the data point can be problematic for the regression model. 
- row 25 and 84 were also noted earlier to be an extreme outier for the wser variable. Thus based on this finding the point will be removed and the regression will be redone. 
- Judging from the residuals vs. fitted plot the model may have some bias when the predicted value crmrte is between 0 to 0.04. Particularly the model tend to underpredict lower crmrates, and overpredict medium crmrte.
- From the Normal Q-Q line, it looks like that majority of predictions follow the line, indicating a normal and independent distribution.

```{R}
#TODO clean out the warning
#std_model <- lm(crmrte ~ . - county-year-crmrte-urban-west-wtrd-wfed-wfir, data =  std_crime_df)

#plot(std_model,1)
#plot(std_model,5)
#plot(std_model,2)

#summary(std_model)$r.squared
```

```
std_crime_df2 <- std_crime_df[-c(84,25),]

std_model2 <- lm(crmrte ~ . - county-year-crmrte-urban-west-wtrd-wfed-wfir, data =  std_crime_df2)

plot(std_model2,1)
plot(std_model2,5)
plot(std_model2,2)

```

TO DO: eliminate.

In order to find which variables are most impactful to crmrte, the marginal R-squared against the standardized coefficients were reviewed. Based on the plots, the following variables were found to have the highest marginal R-squared and absolute slope coefficient:
-prbarr
-prbconv
-polpc
-density
-pctmin80


```

coeff_df = data.frame(summary(std_model)$coefficients)
#summary(std_model)$r.squared

#base R-Squared
base_model <- lm(crmrte~.-county-year-crmrte, data=std_crime_df)
base_r2 <- summary(base_model)$r.squared

#create list of variables for the for-loop
var_names <- colnames(std_crime_df)
remove <- c('county',
            'year',
            'crmrte',
            'urban',
            'west',
            'wtrd',
            'wfed',
            'wfir')
var_names <- var_names[! var_names %in% remove]

#initiate an empty vector to store the marginal R-Squared
var_r2_delta = c()

#loop through the variable names and store the marginal R-Squared
for (i in var_names) {
    fmla <- as.formula(paste("crmrte ~ - crmrte +", paste(var_names[! var_names %in% i], collapse= "+")))
    delta_model <- lm(fmla, data=crime_df)
    r2_delta <- base_r2-summary(delta_model)$r.squared
    var_r2_delta <- c(var_r2_delta, r2_delta)
}

#put the variable and marginal R-squared in a dataframe
mar_r2_df <- data.frame(v1=var_names, v2=var_r2_delta)
colnames(mar_r2_df) <- c('variable', 'marginalr2')

#sort dataframe by marginal R-squared in a descending order
#mar_r2_df <- mar_r2_df[rev(order(mar_r2_df$marginalr2)),]

plot(abs(coeff_df[-c(1),]$Estimate),mar_r2_df$marginalr2)

subset(mar_r2_df, marginalr2 > .04)
```

