---
title: "W203 Lab 3"
author: "Armond Kok, Adam Yang, James De La Torre"
output:
  pdf_document:
    fig_height: 4
    fig_width: 6
  html_document:
    df_print: paged
  'pdf_document:': default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
#opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```



# NOTES

Crime Rate: Include all but county numbers
    • Standardize (min-max normalization), so everything is between 0-1
    • Identify which ones have the strongest slope coefficient.
        ○ Best variable will be the one that has the strongest
    • Look at the marginal change in R^2 for each independent variable (exclude one at a time).
Sort descending for marginal R^2





# Introduction

Our team has been hired by a local political campaign to research North Carolina crime statistics and generate suggestions for policies for reducing crime.

The crime statistics data set being analyzed is a subset of the data used by Cornwell and W. Trumball in their 1994 study.  The data set contains the output variable, `crmrate`, which is crimes committed per capita, and it also contains 24 other variables which will be treated as input variables and potential modulators of the crime rate.

We will attempt to build a linear model that regresses `crmrate` on some key variables in the data set.  We hope to identify variables that can be reasonably assessed as causal with respect to the crime rate.  From our model fidings, we will produce policy proposals which we believe can influence these variables and result in a decrease in crime.  

It is important to note that just because a variable is found to correlate with the crime rate, it does not imply that the variable is useful from the perspective of a political campaign.  We may find variables that cannot be influenced by any political policy or action.  Such variables may improve the predictive ability of our model, but they will not be targeted for change by any policy proposals.


# Exploratory Data Analysis



The data file, `crime_v2.csv` was opened and found to contain 97 rows.  Each row represents data for a single county in North Carlolina.  Immediate inspection of the data revealed a few data cleanup steps were required.

- The last 6 rows of the data set were blanks.  These empty records were deleted.
- One row had values of 1 for both `west` and `central`, placing that county in two regions simultaneously.  It is unknown whether this is possible, but currently there has been no reason to delete this particular row so the data will be kept for now, as evaluation of variable importance is still ongoing.
- The `prbconv` variable, representing the "probability of conviction" was read in as a factor (a cateogorical variable) instead of a numeric variable.  This variable was converted to numeric.

```{R}
library(car)
library(reshape2)
library(ggplot2)

# define dir
mydir<-"C:/Users/ak021523/Documents/GitHub/mids-repos/W203/Homework/w203-Lab3/"

#jim's directory
#mydir<- F:/users/jddel/Documents/DATA_SCIENCE_DEGREE_LAPTOP/W203_Stats/Lab_03/"

# read df
crime_df = read.csv(paste0(mydir,'crime_v2.csv'))

# summarize all vars
summary(crime_df)

str(crime_df)


# get rid of rows with missing values (this only kills the 6 blank rows)
crime_df<-crime_df[complete.cases(crime_df),]

# convert prob of conviction to numeric
crime_df$prbconv <- as.numeric(as.character(crime_df$prbconv))
```


### Outlier Identification


After reviewing the distributions of the different variables, there were 4 variables had outliers, which is defined by anything that is more than Q3 + 1.5 IQR or Q1 - 1.5 IQR: 
- polpc - row 51
- prbarr - row 51
- wser - row 84
- taxpc row 25
After reviewing further, there was no reason for the extreme outliers to be removed from the data set. boxplots of the variables above are shown below.

```{R}

boxplot(crime_df$polpc, main ='polpc')
boxplot(crime_df$prbarr, main ='prbarr')
boxplot(crime_df$wser, main = 'wser')
boxplot(crime_df$taxpc, main = 'taxpc')

# 1.5 IQR from the Q3 = outlier but we can decide which to eliminate
```

### Check for multicolinearity

Build a correlation matrix.
Identify input variables that correlate with one another. 
Choose only one variable from each correlated pair to include in model-building.

```{R}
#TODO - fix matrix sizing

#correlation matrix for top 4 correlation and bottom 4 correlation
cor_dr = cor(crime_df[c('prbarr',
                        'prbpris',
                        'prbconv',
                        'avgsen',
                        'polpc',
                        'density',
                        'taxpc',
                        'west',
                        'central',
                        'urban',
                        'pctmin80',
                        'wcon',
                        'wtuc',
                        'wtrd',
                        'wfir',
                        'wser',
                        'wmfg',
                        'wfed',
                        'wsta',
                        'wloc',
                        'mix',
                        'pctymle')],
              use = "complete.obs")

# Heatmap
ggplot(data = melt(cor_dr, na.rm = TRUE),
       aes(Var2, Var1, fill = value))+
theme_minimal()+ 
geom_tile(color = 'white')+
scale_fill_gradient2(low = 'blue',
                     high = "orange",
                     mid = 'white', 
                     midpoint = 0,
                     limit = c(-1,1),
                     name= 'Correlation')+
theme(axis.text.x = element_text(face='bold',
                                 angle=90,
                                 vjust = 1,
                                 size =8,
                                 hjust = 1),
     axis.text.y = element_text(face='bold',
                                size=8),
     axis.title.x=element_blank(),
     axis.title.y=element_blank())

```
One of the assumptions for multiple OLS regression is to avoid perfect multicollinearity between independent variables. This, however, is not common in practical cases. Less than perfect multicollinearity is a more common problem that will not cause bias in the OLS, but would introduce large variances and covariances. As a result, precise estimation would become difficult so it can be beneficial to remove certain imperfect multicollinearity variables.

After reviewing the correlation matrix in detail, there were 5 pairs of variables that have a somewhat strong correlation to each other (i.e. has correlation > 0.6), which are plotted below. Based on the plots, then the following variables were removed from the final model:
- urban - this is somewhat redundant with density.
- west - west was removed because it is a dummy variable, and pctmin80 is a continuous one which may contain more information for the regression model.
- wtrd, wfed, wfir - wages tend to be higher with density, so density was kept as it can succinctly represent the same information. Below are the scatterplots of the different correlated variables

```{R}
plot(crime_df$urban, crime_df$density)
plot(crime_df$west, crime_df$pctmin80)
plot(crime_df$wtrd, crime_df$wfir)
plot(crime_df$wtrd, crime_df$wfed)
plot(crime_df$wfed, crime_df$wfir)
plot(crime_df$wfed+crime_df$wtrd+crime_df$wfir, crime_df$density)
```
### Standardize
In order to compare the impacts of the different independent variables, the values of those variables needed to be standardized so that the slope coefficients are similar in scale (e.g. if the range of a variable is between 0 and 1, then the coefficient may be larger than that of a variable that ranges between 0-200). For the standardization, the variables were all scaled to range between 0 and 1, based on the min and max values.

```{R}

# make a copy of crime_df for standardizing values
std_crime_df <- cbind(crime_df)

# a function to standardize values (fraction of range)
standardize_values <- function(x){(x-min(x))/(max(x)-min(x))}

# for all columns other than county number, year, and crime rate, standardize between 0 and 1
for (col in 4:ncol(std_crime_df)) {
  std_crime_df[,col] <- standardize_values(std_crime_df[,col])
}

summary(std_crime_df)

```

Now that we have standardized the units of all input variables, we can compute model slope coefficients that will be in comparable units.


### Standardized Regression Model
A multi variable regression model was created using the data set that has been standardized above. 

Then the model was evaluated for potential high leverage/influence data points as well as potential biases.

In review the following findings were noted:
- row 84 and 25 have a high Cook's distance and high standardized residuals, which means the data point can be problematic for the regression model. 
- row 25 and 84 were also noted earlier to be an extreme outier for the wser variable. Thus based on this finding the point will be removed and the regression will be redone. 
- Judging from the residuals vs. fitted plot the model may have some bias when the predicted value crmrte is between 0 to 0.04. Particularly the model tend to underpredict lower crmrates, and overpredict medium crmrte.
- From the Normal Q-Q line, it looks like that majority of predictions follow the line, which is what is desired for the regression model.


```{R}
#TODO clean out the warning
temp = na.omit(std_crime_df)
std_model <- lm(crmrte ~ . - county-year-crmrte-urban-west-wtrd-wfed-wfir, data =  temp)

plot(std_model)

#summary(std_model)$r.squared
```

```{R}
summary(std_model)$coefficients
summary(std_model)$r.squared

```

We look at initial plots and see some outliers on the residual vs leverage plot, exceed Cook's distance >0.5 and 1.0.
Should we delete? just a single value in a col? whole row?  what variable is driving row as an outlier?


Next: compute the model r^2 as we eliminate 1 variable at a time.

```{R}
#base R-Squared
base_model <- lm(crmrte~.-county-year-crmrte, data=std_crime_df)
base_r2 <- summary(base_model)$r.squared

#create list of variables for the for-loop
var_names <- colnames(std_crime_df)
remove <- c('county', 'year','crmrte')
var_names <- var_names[! var_names %in% remove]

#initiate an empty vector to store the marginal R-Squared
var_r2_delta = c()

#loop through the variable names and store the marginal R-Squared
for (i in var_names) {
    fmla <- as.formula(paste("crmrte ~ - crmrte +", paste(var_names[! var_names %in% i], collapse= "+")))
    delta_model <- lm(fmla, data=crime_df)
    r2_delta <- base_r2-summary(delta_model)$r.squared
    var_r2_delta <- c(var_r2_delta, r2_delta)
}

#put the variable and marginal R-squared in a dataframe
mar_r2_df <- data.frame(v1=var_names, v2=var_r2_delta)
colnames(mar_r2_df) <- c('variable', 'marginalr2')

#sort dataframe by marginal R-squared in a descending order
mar_r2_df <- mar_r2_df[rev(order(mar_r2_df$marginalr2)),]
```


Identify variables of interest.  Then, check if any of the identified variables are linearly related to one another.  If you find some that are, then we pick some to remove from the model.





JIM: Comment about histograms of variables. make note of any anomalies in data like invalid values.

JIM: Next talk about correlation matrix (I think we should show this).


JIM QUESTIONS:

- High-level question: Is our general policy to delete a datapoint if it has
  any bad variables? (yes)
- Are the prbarr and prbconv values supposed to be between o and 1? We have some values >1 (1,10).  Do we just delete?
- Our report, should it just be a markdown file, not necessarily R markdown?  otherwise we will have to include every line of R code
- Is it valid for west and central to both be 1 (we have 1 row)?  Is it valid for them both to be zero?



