---
title: "W203 Lab 3"
author: "Armand Kok, Adam Yang, James De La Torre"
output:
  pdf_document:
    fig_height: 4
    fig_width: 6
  html_document:
    df_print: paged
  'pdf_document:': default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
#opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```



# Introduction

Our team has been hired by a local political campaign to provide research on North Carolina crime statistics and generate policy suggestions for reducing crime.

The crime statistics dataset provided for analysis is a subset of the data used by Cornwell and W. Trumball in their 1994 study. The dependent variable, of our study is the crimes commited per capita, given as `crmrate`, while there are 24 other variables in the dataset, each of which can be potential modulators of the crime rate. We aim to build a linear model that regresses `crmrate` on the key variables in the dataset. In particular, we are interested in examining the potential of the following policies in reducing crime rate:
  * Policy to increase the police per capita of a county
  * Policy to implement a more stringent arrest protocol
  * Policy to enhance community outreach in high density and minority communities

In addition, we aim to identify other factors that may reduce crime and attempt to fully explore other possible political strategies. Not all correlating variables will have an actionable solution, though their inclusion in the regression model will contribute to the accuracy of the model.

# Exploratory Data Analysis

The data file, `crime_v2.csv` was opened and found to contain 97 rows.  Each row represents data for a single county in North Carlolina.  Immediate inspection of the data revealed a few data cleanup steps were required.

- The last 6 rows of the data set were blanks.  These empty records were deleted.
- One row had values of 1 for both `west` and `central`, placing that county in two regions simultaneously.  It is unknown whether this is possible, but currently there has been no reason to delete this particular row so the data will be kept for now, as evaluation of variable importance is still ongoing.
- The `prbconv` variable, representing the "probability of conviction" was read in as a factor (a cateogorical variable) instead of a numeric variable.  This variable was converted to numeric.

```{R}
library(car)
library(reshape2)
library(ggplot2)

# Adam's dir
mydir <- "/Users/adamyang/Desktop/w203/Lab3/w203-Lab3/"

# Armand's dir
# mydir<-"C:/Users/ak021523/Documents/GitHub/mids-repos/W203/Homework/w203-Lab3/"

#jim's directory
# mydir<- F:/users/jddel/Documents/DATA_SCIENCE_DEGREE_LAPTOP/W203_Stats/Lab_03/"

# read df
crime_df = read.csv(paste0(mydir,'crime_v2.csv'))

# summarize all vars
summary(crime_df)

str(crime_df)


# get rid of rows with missing values (this only kills the 6 blank rows)
crime_df<-crime_df[complete.cases(crime_df),]

# convert prob of conviction to numeric
crime_df$prbconv <- as.numeric(as.character(crime_df$prbconv))
```


### Outlier Identification

After reviewing the distributions of the different variables, there were 4 variables had outliers, which is defined by anything that is more than Q3 + 1.5 IQR or Q1 - 1.5 IQR: 
- polpc - row 51
- prbarr - row 51
- wser - row 84
- taxpc row 25
After reviewing further, there was no reason for the extreme outliers to be removed from the data set. boxplots of the variables above are shown below.

```{R}

boxplot(crime_df$polpc, main ='polpc')
boxplot(crime_df$prbarr, main ='prbarr')
boxplot(crime_df$wser, main = 'wser')
boxplot(crime_df$taxpc, main = 'taxpc')

# 1.5 IQR from the Q3 = outlier but we can decide which to eliminate
```

### Check for multicolinearity

Build a correlation matrix.
Identify input variables that correlate with one another. 
Choose only one variable from each correlated pair to include in model-building.

```{R}
#TODO - fix matrix sizing

#correlation matrix for top 4 correlation and bottom 4 correlation
cor_dr = cor(crime_df[c('prbarr',
                        'prbpris',
                        'prbconv',
                        'avgsen',
                        'polpc',
                        'density',
                        'taxpc',
                        'west',
                        'central',
                        'urban',
                        'pctmin80',
                        'wcon',
                        'wtuc',
                        'wtrd',
                        'wfir',
                        'wser',
                        'wmfg',
                        'wfed',
                        'wsta',
                        'wloc',
                        'mix',
                        'pctymle')],
              use = "complete.obs")

# Heatmap
ggplot(data = melt(cor_dr, na.rm = TRUE),
       aes(Var2, Var1, fill = value))+
theme_minimal()+ 
geom_tile(color = 'white')+
scale_fill_gradient2(low = 'blue',
                     high = "orange",
                     mid = 'white', 
                     midpoint = 0,
                     limit = c(-1,1),
                     name= 'Correlation')+
theme(axis.text.x = element_text(face='bold',
                                 angle=90,
                                 vjust = 1,
                                 size =8,
                                 hjust = 1),
     axis.text.y = element_text(face='bold',
                                size=8),
     axis.title.x=element_blank(),
     axis.title.y=element_blank())

```
One of the assumptions for multiple OLS regression is to avoid perfect multicollinearity between independent variables. This, however, is not common in practical cases. Less than perfect multicollinearity is a more common problem that will not cause bias in the OLS, but would introduce large variances and covariances. As a result, precise estimation would become difficult so it can be beneficial to remove certain imperfect multicollinearity variables.

After reviewing the correlation matrix in detail, there were 5 pairs of variables that have a somewhat strong correlation to each other (i.e. has correlation > 0.6), which are plotted below. Based on the plots, then the following variables were removed from the final model:
- urban - this is somewhat redundant with density.
- west - west was removed because it is a dummy variable, and pctmin80 is a continuous one which may contain more information for the regression model.
- wtrd, wfed, wfir - wages tend to be higher with density, so density was kept as it can succinctly represent the same information. Below are the scatterplots of the different correlated variables

```{R}
plot(crime_df$urban, crime_df$density)
plot(crime_df$west, crime_df$pctmin80)
plot(crime_df$wtrd, crime_df$wfir)
plot(crime_df$wtrd, crime_df$wfed)
plot(crime_df$wfed, crime_df$wfir)
plot(crime_df$wfed+crime_df$wtrd+crime_df$wfir, crime_df$density)
```
### Standardize Independent Variables

In order to compare the impacts of the different independent variables, the values of those variables needed to be standardized so that the slope coefficients are similar in scale (e.g. if the range of a variable is between 0 and 1, then the coefficient may be larger than that of a variable that ranges between 0-200). For the standardization, the variables were all scaled to range between 0 and 1, based on the min and max values.

```{R}

# make a copy of crime_df for standardizing values
std_crime_df <- cbind(crime_df)

# a function to standardize values (fraction of range)
standardize_values <- function(x){(x-min(x))/(max(x)-min(x))}

# for all columns other than county number, year, and crime rate, standardize between 0 and 1
for (col in 4:ncol(std_crime_df)) {
  std_crime_df[,col] <- standardize_values(std_crime_df[,col])
}

summary(std_crime_df)

```

Now that we have standardized the units of all input variables, we can compute model slope coefficients that will be in comparable units.


### Standardized Regression Model

A multi variable regression model was created using the data set that has been standardized above. 

Then the model was evaluated for potential high leverage/influence data points as well as potential biases.

In review the following findings were noted:
- row 84 and 25 have a high Cook's distance and high standardized residuals, which means the data point can be problematic for the regression model. 
- row 25 and 84 were also noted earlier to be an extreme outier for the wser variable. Thus based on this finding the point will be removed and the regression will be redone. 
- Judging from the residuals vs. fitted plot the model may have some bias when the predicted value crmrte is between 0 to 0.04. Particularly the model tend to underpredict lower crmrates, and overpredict medium crmrte.
- From the Normal Q-Q line, it looks like that majority of predictions follow the line, indicating a normal and independent distribution.

```{R}
#TODO clean out the warning
std_model <- lm(crmrte ~ . - county-year-crmrte-urban-west-wtrd-wfed-wfir, data =  std_crime_df)

plot(std_model,1)
plot(std_model,5)
plot(std_model,2)

#summary(std_model)$r.squared
```

```{R}
std_crime_df2 <- std_crime_df[-c(84,25),]

std_model2 <- lm(crmrte ~ . - county-year-crmrte-urban-west-wtrd-wfed-wfir, data =  std_crime_df2)

plot(std_model2,1)
plot(std_model2,5)
plot(std_model2,2)

```

In order to find which variables are most impactful to crmrte, the marginal R-squared against the standardized coefficients were reviewed. Based on the plots, the following variables were found to have the highest marginal R-squared and absolute slope coefficient:
-prbarr
-prbconv
-polpc
-density
-pctmin80


```{R}
coeff_df = data.frame(summary(std_model)$coefficients)
#summary(std_model)$r.squared

#base R-Squared
base_model <- lm(crmrte~.-county-year-crmrte, data=std_crime_df)
base_r2 <- summary(base_model)$r.squared

#create list of variables for the for-loop
var_names <- colnames(std_crime_df)
remove <- c('county',
            'year',
            'crmrte',
            'urban',
            'west',
            'wtrd',
            'wfed',
            'wfir')
var_names <- var_names[! var_names %in% remove]

#initiate an empty vector to store the marginal R-Squared
var_r2_delta = c()

#loop through the variable names and store the marginal R-Squared
for (i in var_names) {
    fmla <- as.formula(paste("crmrte ~ - crmrte +", paste(var_names[! var_names %in% i], collapse= "+")))
    delta_model <- lm(fmla, data=crime_df)
    r2_delta <- base_r2-summary(delta_model)$r.squared
    var_r2_delta <- c(var_r2_delta, r2_delta)
}

#put the variable and marginal R-squared in a dataframe
mar_r2_df <- data.frame(v1=var_names, v2=var_r2_delta)
colnames(mar_r2_df) <- c('variable', 'marginalr2')

#sort dataframe by marginal R-squared in a descending order
#mar_r2_df <- mar_r2_df[rev(order(mar_r2_df$marginalr2)),]

plot(abs(coeff_df[-c(1),]$Estimate),mar_r2_df$marginalr2)

subset(mar_r2_df, marginalr2 > .04)
```

### Non-Standardized Regressions 
The following is the model that contains almost all available variables as explanatory variables with the exception of variables we excluded due to potential multi-collinearity.

```{R}
crime_df2 <- crime_df[-c(84,25),]

model1 <- lm(crmrte ~ . - county-year-crmrte-urban-west-wtrd-wfed-wfir, data = crime_df2)

summary(model1)$r.squared
summary(model1)$coefficients

```

The following is the model that contains a transformed explanatory variable.

```{R}
model_transform <- lm(crmrte ~ prbarr + log(prbconv) + density, data = crime_df2)

summary(model_transform)$r.squared
summary(model_transform)$coefficients
```

The following is the model that contains only variables that were identified to be most relevant to crmrte based on their marginal R-squared and standardized slope coefficient values.

```{R}
model_key <- lm(crmrte ~ prbarr + prbconv + polpc + density + pctmin80, data = crime_df2)

summary(model_key)$r.squared
summary(model_key)$coefficients
```

### Stargazer Regression Table for Model Specifications

```{R, results='asis'}
library(stargazer)
stargazer(model_transform, model_key, model1,
          title = "Linear Models Parameters Predicting Crime Rate",
          type = 'text', report = 'vc',
          keep.stat = c('rsq', 'n'),
          omit.table.layout = 'n')
```


### Recommendation

For interpretability purposes, the model was re-done using non-standardized variables:
-prbarr
-prbconv
-polpc
-density
-pctmin80

Recommendation for political campaign:
- police per capita has a positive slope coefficient with crmrte, and this may be due to more police are present in areas with high crmrte. This suggests that purely hiring more police officers may not be an impactful solution.
- However probability of arrest and conviction both have a negative slope coefficients. The model suggests that perhaps a zero tolerance policy towards crime is needed to increase arrests and convictions and thus deter crimes from happening.
- In terms areas with large minority population and high density, since these variable cannot be changed that much, perhaps a community outreach (e.g. job training program, afterschool programs, tutor/mentor program) to educate areas with a lot of minority can be done, so that crimes can be reduced in those areas.


### Omitted Variables

Potential Omitted Variable #1: poverty_rate
$$ crmrte = \beta_0 + \beta_1*density + \beta_2*poverty\_rate + u$$
$$ poverty\_rate = \alpha_0 + \alpha_1*density +u $$

- One thing that was noticeable in  the data is that crmrate was higher in dense areas and large minority population, however this may be due to an omitted variable that is not available in the data set. 
- For example: in dense areas the cost of living may be much higher, which can explain why higher wages are correlated with dense areas, but because of the higher cost of living. Because of this, there may be a lot more people living under the poverty line, which would encourage them to commit crimes and hence why dense areas have higher crmrte.
- so the density slope coefficient in this instance is probably higher than it should be $\beta_2$ and $\alpha_1$ would be positive.
- Maybe tax revenue or wages can help proxy this omitted variable.

Potential Omitted Variable #2: discrimination
$$ crmrte = \beta_0 + \beta_1*pctmin80 + \beta_2*discrimination $$
$$ discrimination = \alpha_0 + \alpha_1*pctmin80 $$
- Similarly minorities may be arrested for crimes more often than necessary due to discrimination. 
- in this scenario $\beta_2$ and $alpha_1$ would be a positive value.

Potential Omitted Variable #3: raised_in_oneparent_hh
$$ crmrte = \beta_0 + \beta_1*pctmin80 + \beta_2*raised\_in\_2parents\_hh $$
$$ raised\_in\_2parents\_hh = \alpha_0 + \alpha_1*pctmin80 $$
- In this scenario, minorities may be more likely to be raised in a single parent house hold. Thus making them more likely to commit crimes. 
- $\beta_2$ would be positive and $\alpha_1$ would be negative.

Potential Omitted Variable #4: unemployment

$$ crmrte = \beta_0 + \beta_1*density + \beta_2*unemployment $$

$$ unemployment = \alpha_0 + \alpha_1*density $$

- Higher umployment = higher crime rate (beta2 > 0)
- Higher density = higher unemployment (alpha1 > 0)
- beta1 was positive, therefore, it might be higher than it should've been.

Potential Omitted Variable #5: years_of_education

$$ crmrte = \beta_0 + \beta_1*pctmin80 + \beta_2*years\_of\_education $$
$$ years\_of\_education = \alpha_0 + \alpha_1*pctmin80 $$
- Higher avg years of education for a county would result in lower crime rate, beta2 < 0
- Higher percentage of minorities = lower average years of education for a county, alpha1 <0
- $\beta_2*\alpha_1 > 0$, beta1 >0, therefore, it might be higher than it should've been.
